import{_ as t,c as i,b as l,o as n}from"./app-i0cLEam0.js";const o={};function a(r,e){return n(),i("div",null,[...e[0]||(e[0]=[l('<h1 id="吴恩达机器学习" tabindex="-1">吴恩达机器学习</h1><h1 id="目录" tabindex="-1">目录</h1><h1 id="几种最优化算法" tabindex="-1">几种最优化算法</h1><p>另参考：</p><ul><li><a href="https://blog.csdn.net/qq_40626659/article/details/103074035" target="_blank" rel="noopener noreferrer">【CSDN】梯度下降法和牛顿法</a></li><li><a href="https://blog.csdn.net/weixin_30947043/article/details/99309191" target="_blank" rel="noopener noreferrer">【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）</a><ul><li><a href="https://www.cnblogs.com/maybe2030/p/4751804.html" target="_blank" rel="noopener noreferrer">【博客园】【Math】常见的几种最优化方法</a></li></ul></li></ul><h2 id="几种最优化算法-1" tabindex="-1">几种最优化算法</h2><p>常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等</p><h3 id="梯度下降法-gd-gradient-descent" tabindex="-1">梯度下降法（GD，Gradient Descent）</h3><h4 id="梯度下降法-gd" tabindex="-1">梯度下降法（GD）</h4><h4 id="批量梯度下降-bgd" tabindex="-1">批量梯度下降（BGD）</h4><p>在梯度下降的每一步中，我们都用到了所有的训练样本，需要进行求和运算</p><h4 id="随机梯度下降-sgd" tabindex="-1">随机梯度下降（SGD）</h4><p>SGD和BGD相反，SGD每次更新参数仅用一个样本进行，而BGD是用所有样本。</p><h4 id="小批量梯度下降-mbgd" tabindex="-1">小批量梯度下降（MBGD）</h4><h4 id="【比较】各种版本的梯度下降" tabindex="-1">【比较】各种版本的梯度下降</h4><h3 id="牛顿法-newton-s-method" tabindex="-1">牛顿法（Newton&#39;s Method）</h3><h4 id="拟牛顿法-quasi-newton-methods" tabindex="-1">拟牛顿法（Quasi-Newton Methods）</h4><h2 id="牛顿法和梯度下降法" tabindex="-1">牛顿法和梯度下降法</h2><p>先来比较 牛顿法和梯度下降法</p><ul><li><p>梯度下降（Gradient Descent）</p><ul><li><p>优点：最简单 最常用 最直观、实现简单，也被称为是 “最速下降法”（当然不意味着速度最快）</p></li><li><p>原理</p><blockquote><p>是用于求函数最小值的算法，其步骤为：</p><ul><li>随机选择一个参数组合，计算损失函数</li><li>通过方向和补仓，对参数进行更新，找下一个能够让损失函数值更低的参数组合</li><li>持续迭代直至寻找到一个局部最小值。</li></ul><p>因为没有尝试所有的参数组合，所以不能保证寻找到的局部最小值就是全局最小值</p></blockquote></li><li><p>分类</p><ul><li>批量梯度下降（BGD）：在梯度下降的每一步中，我们都用到了所有的训练样本，需要进行求和运算</li><li>随机梯度下降（SGD）：SGD和BGD相反，SGD每次更新参数仅用一个样本进行，而BGD是用所有样本。</li><li>小批量梯度下降（MBGD）：是BGD和SGD的中和，每次参数迭代用大于一小于所有的样本。其优点为：比SGD精度高，但可能需要的时间比较长。</li></ul></li></ul></li><li><p>牛顿法和拟牛顿法（Newton&#39;s method &amp; Quasi-Newton Methods）</p><ul><li><p>原理</p><blockquote><p>其实质是对 损失函数 进行求导，寻找能使得损失函数导数为0的解，当损失函数导数为0时，也便找到了最优解（极值点）</p><p>其迭代过程是在当前位置x0求该函数的切线，该切线和x轴的交点x1，作为新的x0。</p><p>重复这个过程，直到交点和函数的零点重合。此时的参数值就是使得目标函数取得极值的参数值</p></blockquote></li></ul></li><li><p>比较</p><ul><li><p>收敛速度</p><p>收敛速度上，牛顿法要比梯度下降法更快，因为其参数更新的步长会比较大，<br> 在接近最优解时，梯度下降法也容易因步长比较大而产生来回震荡的效果，从而降低了收敛速度。</p></li><li><p>计算量</p><p>但是在运行过程中，牛顿法的计算量要远大于梯度下降，因为牛顿法要对多个值进行求导运算，而梯度下降仅需要得出方向和步长便能更新参数</p></li></ul></li></ul>',20)])])}const d=t(o,[["render",a]]),c=JSON.parse('{"path":"/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/%E7%BA%BF%E6%80%A7%E5%9E%8B/01.%20%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.%20%E8%A1%A5%E5%85%85%E9%A1%B9/01.%20%E5%87%A0%E7%A7%8D%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html","title":"吴恩达机器学习","lang":"zh-CN","frontmatter":{"description":"吴恩达机器学习 目录 几种最优化算法 另参考： 【CSDN】梯度下降法和牛顿法 【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等） 【博客园】【Math】常见的几种最优化方法 几种最优化算法 常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等 梯度下降法（GD，Gradient Descent） 梯度下降法（G...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"吴恩达机器学习\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-28T10:04:33.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://LincDocs.github.io/MdNote_Public/01.%20DesignAndDevelop/Develop/04.%20Project/Type/Artificial_Intelligence/%E7%BA%BF%E6%80%A7%E5%9E%8B/01.%20%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.%20%E8%A1%A5%E5%85%85%E9%A1%B9/01.%20%E5%87%A0%E7%A7%8D%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html"}],["meta",{"property":"og:site_name","content":"MdNote_Public"}],["meta",{"property":"og:title","content":"吴恩达机器学习"}],["meta",{"property":"og:description","content":"吴恩达机器学习 目录 几种最优化算法 另参考： 【CSDN】梯度下降法和牛顿法 【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等） 【博客园】【Math】常见的几种最优化方法 几种最优化算法 常见的几种最优化算法：梯度下降法、牛顿法、拟牛顿法、共轭梯度法等 梯度下降法（GD，Gradient Descent） 梯度下降法（G..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-09-28T10:04:33.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-28T10:04:33.000Z"}]]},"git":{"createdTime":1759053873000,"updatedTime":1759053873000,"contributors":[{"name":"Linc","username":"Linc","email":"762699299@qq.com","commits":1,"url":"https://github.com/Linc"}]},"readingTime":{"minutes":2.73,"words":819},"filePathRelative":"01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/线性型/01. 吴恩达 机器学习/10. 补充项/01. 几种最优化算法.md","excerpt":"\\n<h1>目录</h1>\\n<h1>几种最优化算法</h1>\\n<p>另参考：</p>\\n<ul>\\n<li><a href=\\"https://blog.csdn.net/qq_40626659/article/details/103074035\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">【CSDN】梯度下降法和牛顿法</a></li>\\n<li><a href=\\"https://blog.csdn.net/weixin_30947043/article/details/99309191\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">【CSDN】常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）</a>\\n<ul>\\n<li><a href=\\"https://www.cnblogs.com/maybe2030/p/4751804.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">【博客园】【Math】常见的几种最优化方法</a></li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true,"bioChainData":{"outlink":[],"backlink":[],"localMap":{"nodes":[{"id":"01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/线性型/01. 吴恩达 机器学习/10. 补充项/01. 几种最优化算法.md","value":{"title":"01. 几种最优化算法","path":"01. DesignAndDevelop/Develop/04. Project/Type/Artificial_Intelligence/线性型/01. 吴恩达 机器学习/10. 补充项/01. 几种最优化算法.md","outlink":[],"backlink":[]}}],"links":[]}}}');export{d as comp,c as data};
